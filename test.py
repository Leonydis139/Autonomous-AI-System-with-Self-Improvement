Error Handling - Use more specific exceptions where possible instead of general Exception. This can improve clarity and debugging. - Return appropriate error codes and messages to help with client debugging. # type: ignore ### 2. Response Formatting - Ensure that all API responses are consistently wrapped in JSON objects, even for simple responses. ### 3. Concurrency and Performance - Consider using asynchronous programming with FastAPI to handle requests concurrently. - Optionally, enable caching for repeated requests. ### 4. Agent Interface Enhancements - Add more detailed logging that includes session context for better traceability. - Introduce performance metrics for each agent step. ### 5. Type Annotations and Comments - Add type hints to all functions for better clarity and usage of IDE features. - Add more comments where logic may not be immediately clear. ### 6. Gradio Enhancements - Improve the user interface with additional components and layouts for better usability. - Add clear instructions on the UI for user guidance. ### Enhanced Code Implementation Here's an improved version of your code reflecting the above suggestions: ```python from fastapi import FastAPI, HTTPException from fastapi.responses import JSONResponse from pydantic import BaseModel, constr from memory import add_to_memory, get_memory, get_summary from ctransformers import AutoModelForCausalLM from src.core.cognitive_engine import CognitiveEngine from src.agents.planner import plan_task from src.agents.executor import execute_step from src.agents.critic import review_result import uuid import time import logging import gradio as gr from typing import Any, List, Dict # Initialize FastAPI app and logging app = FastAPI() logging.basicConfig(filename="log.txt", level=logging.INFO) # Load the language model llm_model = AutoModelForCausalLM.from_pretrained( "TheBloke/zephyr-7B-alpha-GGUF", model_file="zephyr-7b-alpha.Q4_K_M.gguf", model_type="llama", max_new_tokens=256, temperature=0.7, ) cognitive_engine = CognitiveEngine(llm_model) class GenerateRequest(BaseModel): prompt: constr(min_length=5, max_length=1000) session_id: constr(min_length=3, max_length=50) @app.get("/status") def status() -> Dict[str, Any]: """Returns API status and available routes.""" return { "message": "âœ… Autonomous AI API is live", "routes": [ "/status", "/generate?prompt=...", "/generate [POST]", "/memory/{session_id}", "/summary/{session_id}", ], } @app.get("/generate") def generate(prompt: str) -> JSONResponse: """Generates a response for the given prompt.""" try: response = llm_model(prompt) return JSONResponse(content={"response": response}) except ValueError as e: logging.error(f"Value error: {str(e)}") raise HTTPException(status_code=400, detail="Invalid input.") except Exception as e: logging.error(f"Error generating response: {str(e)}") raise HTTPException(status_code=500, detail="Failed to generate response.") @app.post("/generate") def generate_post(data: GenerateRequest) -> Dict[str, Any]: """Generates a response and logs the process with latency.""" start = time.time() try: response = llm_model(data.prompt) latency = round(time.time() - start, 2) add_to_memory(data.session_id, data.prompt, response) logging.info(f"[{data.session_id}] Prompt processed in {latency}s") return {"session_id": data.session_id, "response": response, "latency": latency} except ValueError as e: logging.error(f"[{data.session_id}] Value error: {str(e)}") raise HTTPException(status_code=400, detail="Invalid input.") except Exception as e: logging.error(f"[{data.session_id}] Error processing prompt: {str(e)}") raise HTTPException(status_code=500, detail="Error processing prompt.") @app.get("/memory/{session_id}") def session_memory(session_id: str) -> Dict[str, Any]: """Fetches the memory history for a specific session.""" try: history = get_memory(session_id) return {"session_id": session_id, "history": history} except KeyError: raise HTTPException(status_code=404, detail="Memory not found.") except Exception as e: logging.error(f"Error fetching memory for {session_id}: {str(e)}") raise HTTPException(status_code=500, detail="Error fetching memory.") @app.get("/summary/{session_id}") def memory_summary(session_id: str) -> Dict[str, Any]: """Fetches a summary of memory for a specific session.""" try: summary = get_summary(session_id) return {"session_id": session_id, "summary": summary} except KeyError: raise HTTPException(status_code=404, detail="Summary not found.") except Exception as e: logging.error(f"Error fetching summary for {session_id}: {str(e)}") raise HTTPException(status_code=500, detail="Error fetching summary.") def agent_interface(goal: str) -> List[Dict[str, Any]]: """Handles the agent's interface and execution of goals.""" session_id = str(uuid.uuid4())[:8] context = get_summary(session_id) plan = plan_task(goal, context) output = [] for step in plan: result = execute_step(step) review = review_result(step, result) output.append({ "step": step, "result": result, "review": review, }) logging.info(f"[{session_id}] Executed step: {step}, Result: {result}, Review: {review}") return output # Gradio UI setup with gr.Blocks() with gr.Blocks() as demo: gr.Markdown("# Multi-Agent Interface") with gr.Tab("Agent Task Runner"): inp = gr.Textbox(label="Task", placeholder="Describe your goal", lines=2) out = gr.Json(label="Agent Output") btn = gr.Button("Run") btn.click(agent_interface, inputs=[inp], outputs=[out]) gr.mount_gradio_app(app, demo, path="/") if __name__ == "__main__": import uvicorn uvicorn.run(app, host="0.0.0.0", port=7860)

